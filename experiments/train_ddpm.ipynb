{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azhgh22/Comparative-analysis-of-Generative-models-on-CIFAR-10/blob/main/experiments/train_ddpm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Env**"
      ],
      "metadata": {
        "id": "Z9tffE4uHlFb"
      },
      "id": "Z9tffE4uHlFb"
    },
    {
      "cell_type": "code",
      "id": "adfc8c40",
      "metadata": {
        "id": "adfc8c40"
      },
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import userdata\n",
        "token = userdata.get('GITHUB_TOKEN')\n",
        "user_name = userdata.get('GITHUB_USERNAME')\n",
        "mail = userdata.get('GITHUB_MAIL')\n",
        "\n",
        "!git config --global user.name \"{user_name}\"\n",
        "!git config --global user.email \"{mail}\"\n",
        "!git clone https://{token}@github.com/azhgh22/Comparative-analysis-of-Generative-models-on-CIFAR-10.git"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "zqJ8qRl4HpkB"
      },
      "id": "zqJ8qRl4HpkB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Add the root directory of the cloned repository to the Python path\n",
        "sys.path.append('/content/Comparative-analysis-of-Generative-models-on-CIFAR-10')\n",
        "\n",
        "import importlib\n",
        "import data.cifar10 as cifar10_module\n",
        "importlib.reload(cifar10_module)\n",
        "from data.cifar10 import load_cifar10\n",
        "\n"
      ],
      "metadata": {
        "id": "EBoRoK5BHczO"
      },
      "id": "EBoRoK5BHczO",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, _ = load_cifar10(batch_size=1024, normalize_inputs=True, pin_memory=True, num_workers=2)"
      ],
      "metadata": {
        "id": "3lEwe0vYIHl2"
      },
      "id": "3lEwe0vYIHl2",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ead33c53"
      },
      "source": [
        "print(f\"Batch Size: {train_loader.batch_size}\")\n",
        "print(f\"Num Workers: {train_loader.num_workers}\")\n",
        "print(f\"Pin Memory: {train_loader.pin_memory}\")\n",
        "\n",
        "# Tip: If num_workers is 0, try increasing it to 2 or 4 to parallelize data loading.\n",
        "# Tip: pin_memory=True speeds up transfer to GPU."
      ],
      "id": "ead33c53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65e80fb5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert to HWC for plotting\n",
        "def show_img(img):\n",
        "  img = (img + 1) / 2\n",
        "  img = torch.clamp(img, 0, 1)\n",
        "\n",
        "  if img.dim() == 4:\n",
        "      img = img[0]\n",
        "  img = img.detach().cpu()\n",
        "  img = img.clamp(0,1)\n",
        "  img = img.permute(1,2,0)  # CHW -> HWC\n",
        "  plt.figure(figsize=(4,4))\n",
        "  plt.imshow(img, interpolation='nearest')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "id": "65e80fb5",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images, title=\"Images\", n_row=4):\n",
        "    # Display a grid of images\n",
        "    images = images.cpu()\n",
        "    # Denormalize from [-1, 1] to [0, 1]\n",
        "    images = (images + 1) / 2\n",
        "    images = torch.clamp(images, 0, 1)\n",
        "\n",
        "    n = len(images)\n",
        "    n_col = n_row\n",
        "    n_row = (n + n_col - 1) // n_col\n",
        "\n",
        "    fig, axes = plt.subplots(n_row, n_col, figsize=(n_col * 2, n_row * 2))\n",
        "    axes = axes.flatten() if n > 1 else [axes]\n",
        "\n",
        "    for i, ax in enumerate(axes):\n",
        "        if i < n:\n",
        "            img = images[i].permute(1, 2, 0).numpy()\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "        else:\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NN5JWwMjcB6O"
      },
      "id": "NN5JWwMjcB6O",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "45bd30a99e016143"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "train_dataset = train_loader.dataset\n",
        "img = train_dataset[0][0]\n",
        "show_img(img)"
      ],
      "id": "45bd30a99e016143"
    },
    {
      "metadata": {
        "id": "e14733f07e87c733"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from models.scorebased_models.ddpm import create_ddpm\n",
        "from utils.get_device import get_device\n",
        "from train.train import Train\n",
        "from utils.checkpointer import Checkpointer\n",
        "\n",
        "device = get_device()\n",
        "\n",
        "model = create_ddpm(image_size=32, image_channels=3, timesteps=1000).to(device)"
      ],
      "id": "e14733f07e87c733"
    },
    {
      "metadata": {
        "id": "4fb3b7de74407"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "checkpoint_dir = \"/content/drive/MyDrive/checkpoints_final/ddpm\"\n",
        "checkpointer = Checkpointer(checkpoint_dir, \"ddpm\", 10, False)\n",
        "train = Train(model, 200, train_loader, checkpointer, device)\n",
        "train.load_checkpoint()"
      ],
      "id": "4fb3b7de74407"
    },
    {
      "metadata": {
        "id": "c996bec7e5a1e1c4",
        "collapsed": true
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "train.train()"
      ],
      "id": "c996bec7e5a1e1c4"
    },
    {
      "metadata": {
        "id": "3531b1b385a0aaae"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "print(model.model.conv_in.weight.grad.mean())\n",
        "print(model.model.conv_in.weight.grad.std())\n",
        "print(model.model.conv_in.weight.grad.abs().mean())"
      ],
      "id": "3531b1b385a0aaae"
    },
    {
      "metadata": {
        "id": "f0f06e107f042f3f"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "gen_img = model.sample(16)"
      ],
      "id": "f0f06e107f042f3f"
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(gen_img)"
      ],
      "metadata": {
        "id": "oDz6VsR95YX9"
      },
      "id": "oDz6VsR95YX9",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d85009d9"
      },
      "source": [
        "### Check System RAM and GPU RAM Usage"
      ],
      "id": "d85009d9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa27d3e7",
        "outputId": "26cce36d-3633-430e-8798-898fd2ff7022"
      },
      "source": [
        "print('--- System RAM Usage ---')\n",
        "!free -h\n",
        "\n",
        "print('\\n--- GPU RAM Usage ---')\n",
        "!nvidia-smi"
      ],
      "id": "fa27d3e7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- System RAM Usage ---\n",
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            12Gi       2.3Gi       1.6Gi       161Mi       8.8Gi       9.9Gi\n",
            "Swap:             0B          0B          0B\n",
            "\n",
            "--- GPU RAM Usage ---\n",
            "Fri Jan 30 23:49:04 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0             30W /   70W |   11360MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "217afce2"
      },
      "source": [
        "Understanding the output from the above cells will help pinpoint whether it's CPU or GPU memory that is constrained. Here are some common solutions:\n",
        "\n",
        "1.  **Reduce Batch Size**: If your batch size is large (e.g., 128 as currently set for your `train_loader`), try reducing it. A smaller batch size uses less memory per iteration.\n",
        "\n",
        "2.  **Delete Unused Variables**: Explicitly delete variables or tensors that are no longer needed, especially large ones. Python's garbage collector might not always immediately free memory, so forcing it can help.\n",
        "\n",
        "3.  **Clear CUDA Cache**: If your GPU memory is full, you can try clearing the CUDA cache. This often helps if there are fragmented or lingering allocations.\n",
        "\n",
        "4.  **Restart Runtime**: This is the most straightforward way to free up all allocated memory. If you restart the runtime, you'll need to re-run your setup cells.\n",
        "\n",
        "5.  **Optimize Data Loading**: Ensure your `load_cifar10` function or any data transformations aren't loading the entire dataset into CPU memory unnecessarily or creating many copies.\n",
        "\n",
        "6.  **Gradient Accumulation (for GPU memory)**: If you need a larger 'effective' batch size but are constrained by GPU memory, you can use gradient accumulation by performing forward/backward passes on smaller batches and only updating weights after several steps."
      ],
      "id": "217afce2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "052acd23"
      },
      "source": [],
      "id": "052acd23",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}