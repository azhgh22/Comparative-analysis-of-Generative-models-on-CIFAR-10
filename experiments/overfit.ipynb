{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NCSN Overfitting Test\n",
    "This notebook tests if the NCSN model can overfit on a small subset of data.\n",
    "If the model can overfit, it confirms that:\n",
    "1. Backpropagation is working correctly\n",
    "2. The model has sufficient capacity\n",
    "3. The loss function is properly defined\n"
   ],
   "id": "eed1b471a30ca66f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from models.scorebased_models.ncsn import NCSN\n",
    "from utils.get_device import get_device\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Load a Tiny Subset of CIFAR-10\n",
    "We'll use only 16 images to test overfitting."
   ],
   "id": "53f14c90e0749f50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data.cifar10 import load_cifar10\n",
    "\n",
    "full_dataset, _ = load_cifar10(batch_size=32, data_dir='./../data')\n",
    "\n",
    "# Select only 16 images for overfitting test\n",
    "NUM_SAMPLES = 16\n",
    "indices = list(range(NUM_SAMPLES))\n",
    "tiny_dataset = Subset(full_dataset.dataset, indices)\n",
    "\n",
    "# Create dataloader with batch size = NUM_SAMPLES (single batch)\n",
    "tiny_loader = DataLoader(tiny_dataset, batch_size=NUM_SAMPLES, shuffle=False)\n",
    "\n",
    "# Get the single batch for visualization\n",
    "for batch_images, batch_labels in tiny_loader:\n",
    "    test_batch = batch_images.to(device)\n",
    "    break\n",
    "\n",
    "print(f\"Test batch shape: {test_batch.shape}\")\n",
    "print(f\"Test batch range: [{test_batch.min().item():.3f}, {test_batch.max().item():.3f}]\")"
   ],
   "id": "bd63e60113373701",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Visualize the Target Images",
   "id": "2cb5ff2196858ad8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def show_images(images, title=\"Images\", nrow=4):\n",
    "    # Display a grid of images\n",
    "    images = images.cpu()\n",
    "    # Denormalize from [-1, 1] to [0, 1]\n",
    "    # images = (images + 1) / 2\n",
    "    images = torch.clamp(images, 0, 1)\n",
    "\n",
    "    n = len(images)\n",
    "    ncol = nrow\n",
    "    nrow = (n + ncol - 1) // ncol\n",
    "\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=(ncol * 2, nrow * 2))\n",
    "    axes = axes.flatten() if n > 1 else [axes]\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < n:\n",
    "            img = images[i].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_images(test_batch, \"Target Images (Ground Truth)\", nrow=4)\n"
   ],
   "id": "17b75b38cef7680d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Initialize NCSN Model with Smaller Architecture\n",
    "Using a smaller model for faster overfitting test."
   ],
   "id": "42bd8e2d18695d45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create model with smaller architecture for faster training\n",
    "model = NCSN(\n",
    "    channels=64,        # Reduced from 128\n",
    "    num_scales=10,\n",
    "    image_size=32,\n",
    "    in_channels=3,\n",
    "    lr=1e-3,           # Higher learning rate for faster overfitting\n",
    "    sigma_min=0.01,\n",
    "    sigma_max=1.0,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Sigma schedule: {model.sigmas}\")"
   ],
   "id": "e2715741edf52551",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Training Loop - Overfit on 16 Images\n",
    "We expect the loss to decrease significantly if backpropagation is working."
   ],
   "id": "4c99ae3eb0b15558"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "NUM_EPOCHS = 2000\n",
    "losses = []\n",
    "\n",
    "print(\"Starting overfitting test...\")\n",
    "print(f\"Training on {NUM_SAMPLES} images for {NUM_EPOCHS} epochs\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train on the single batch repeatedly\n",
    "    loss_dict = model.train_step(test_batch, epoch)\n",
    "    loss_value = loss_dict['total_loss']\n",
    "    losses.append(loss_value)\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Loss: {loss_value:.6f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ],
   "id": "dfa0d22562ef8395",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.5. Gradient Analysis\n",
    "Check gradient flow and learning dynamics\n"
   ],
   "id": "6ffc9bdb8658ecf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test gradient flow\n",
    "model.score_net.train()\n",
    "model.optimizer.zero_grad()\n",
    "\n",
    "# Compute loss on the batch\n",
    "loss, indices = model.compute_loss(test_batch)\n",
    "loss.backward()\n",
    "\n",
    "# Analyze gradients\n",
    "print(\"Gradient Statistics:\")\n",
    "print(\"-\" * 60)\n",
    "total_norm = 0.0\n",
    "for name, param in model.score_net.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        param_norm = param.grad.data.norm(2).item()\n",
    "        total_norm += param_norm ** 2\n",
    "        if 'conv_out' in name or 'embed' in name or 'conv_in' in name:\n",
    "            print(f\"{name:40s} | grad norm: {param_norm:.6f}\")\n",
    "\n",
    "total_norm = total_norm ** 0.5\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total gradient norm: {total_norm:.6f}\")\n",
    "print()\n",
    "\n",
    "# Check if gradients are too large or too small\n",
    "if total_norm > 1000:\n",
    "    print(\"‚ö† WARNING: Gradients are very large! This may cause instability.\")\n",
    "    print(\"   Consider: gradient clipping, lower learning rate, or check loss scale\")\n",
    "elif total_norm < 0.01:\n",
    "    print(\"‚ö† WARNING: Gradients are very small! Learning may be slow.\")\n",
    "    print(\"   Consider: higher learning rate or check if weights are initialized properly\")\n",
    "else:\n",
    "    print(\"‚úì Gradient norms look reasonable\")\n",
    "print()\n",
    "\n",
    "# Check loss scale\n",
    "print(f\"Current loss value: {loss.item():.6f}\")\n",
    "print(f\"Loss per pixel: {loss.item() / (32 * 32 * 3):.6f}\")\n"
   ],
   "id": "5035d2aaf8068bad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.6. Loss Computation Deep Dive\n",
    "Verify the loss calculation is correct"
   ],
   "id": "1e0785135e5d0c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Loss Computation Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Manual loss calculation\n",
    "model.score_net.eval()\n",
    "with torch.no_grad():\n",
    "    # Take one image\n",
    "    x_test = test_batch[0:1]\n",
    "\n",
    "    # Test with different sigmas\n",
    "    for i, sigma_idx in enumerate([0, 4, 9]):\n",
    "        sigma = model.sigmas[sigma_idx]\n",
    "        print(f\"\\nSigma index {sigma_idx}, œÉ = {sigma.item():.4f}:\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # Add noise\n",
    "        noise = torch.randn_like(x_test)\n",
    "        perturbed = x_test + sigma * noise\n",
    "\n",
    "        # Get prediction\n",
    "        sigma_input = torch.ones(1, device=device) * sigma\n",
    "        predicted_score = model.score_net(perturbed, sigma_input)\n",
    "\n",
    "        # Target score\n",
    "        target_score = -noise / sigma\n",
    "\n",
    "        # Compute loss components\n",
    "        error = predicted_score - target_score\n",
    "        unweighted_mse = (error ** 2).mean()\n",
    "        weighted_loss = 0.5 * ((error ** 2).sum() * (sigma ** 2))\n",
    "\n",
    "        print(f\"  Noise std: {noise.std().item():.6f}\")\n",
    "        print(f\"  Target score std: {target_score.std().item():.6f}\")\n",
    "        print(f\"  Predicted score std: {predicted_score.std().item():.6f}\")\n",
    "        print(f\"  Unweighted MSE: {unweighted_mse.item():.6f}\")\n",
    "        print(f\"  Weighted loss: {weighted_loss.item():.6f}\")\n",
    "        print(f\"  Loss scale (œÉ¬≤): {(sigma ** 2).item():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Analysis:\")\n",
    "print(\"The loss is weighted by œÉ¬≤. Large œÉ values lead to large losses.\")\n",
    "print(\"This explains why the absolute loss values are in thousands.\")\n",
    "print(\"What matters is the RELATIVE change and stability during training.\")\n"
   ],
   "id": "92d97cda92ad721d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Plot Loss Curve\n",
    "A decreasing loss indicates successful overfitting and confirms backpropagation is working.\n"
   ],
   "id": "91364a7a9092ea3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (Full Range)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {losses[0]:.6f}\")\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "print(f\"Loss reduction: {(losses[0] - losses[-1]) / losses[0] * 100:.2f}%\")\n"
   ],
   "id": "cbb91f74cc9e544c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Test Score Predictions\n",
    "Visualize the predicted scores for different noise levels."
   ],
   "id": "4338f27f406be267"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate a single image for visualization\n",
    "gen_img = model.sample(batch_size=1)\n"
   ],
   "id": "530be05dd6c7607e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Generated image shape: {gen_img.shape}\")\n",
    "print(f\"Generated image range: [{gen_img.min().item():.3f}, {gen_img.max().item():.3f}]\")\n",
    "\n",
    "# Plot the single generated image\n",
    "gen_img_cpu = gen_img[0].cpu()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(gen_img_cpu.permute(1, 2, 0))\n",
    "plt.title(\"Generated Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "id": "75f2cca398e8e288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.sigmas.sort()",
   "id": "a59d2b7f48e12caf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gen_img_cpu",
   "id": "9b977a3f19885047",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate multiple samples for grid visualization\n",
    "gen_imgs = model.sample(batch_size=16, n_steps=100, eps=2e-5)\n",
    "print(f\"Generated images shape: {gen_imgs.shape}\")\n",
    "\n",
    "# Display grid of generated images\n",
    "show_images(gen_imgs, \"Generated Images Grid\", nrow=4)\n"
   ],
   "id": "97c8022cfb87270e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Generate Samples Using Annealed Langevin Dynamics\n",
    "Test if the model can reconstruct similar images after overfitting.\n"
   ],
   "id": "4cba66d67bc7dc44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Generating samples...\")\n",
    "\n",
    "# Generate samples\n",
    "samples = model.sample(batch_size=16, n_steps=100, eps=2e-5)\n",
    "\n",
    "print(f\"Generated samples shape: {samples.shape}\")\n",
    "print(f\"Generated samples range: [{samples.min().item():.3f}, {samples.max().item():.3f}]\")\n",
    "\n",
    "show_images(samples, \"Generated Samples After Overfitting\", nrow=4)\n"
   ],
   "id": "e2954cbb243a5dac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. side-by-side comparison",
   "id": "57e4f80260ad0907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    orig = (test_batch[i].cpu() + 1) / 2\n",
    "    axes[0, i].imshow(orig.permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Original', fontsize=12)\n",
    "\n",
    "    # Generated\n",
    "    gen = (samples[i].cpu() + 1) / 2\n",
    "    axes[1, i].imshow(gen.permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Generated', fontsize=12)\n",
    "\n",
    "plt.suptitle('Original vs Generated (After Overfitting)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3e78a0b70ba2fefc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Analysis and Conclusions",
   "id": "a98383b796b540b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"OVERFITTING TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "loss_reduction = (losses[0] - losses[-1]) / losses[0] * 100\n",
    "\n",
    "print(f\"1. Loss Analysis:\")\n",
    "print(f\"   - Initial loss: {losses[0]:.6f}\")\n",
    "print(f\"   - Final loss: {losses[-1]:.6f}\")\n",
    "print(f\"   - Reduction: {loss_reduction:.2f}%\")\n",
    "\n",
    "if loss_reduction > 50:\n",
    "    print(\"   ‚úì PASS: Significant loss reduction indicates backpropagation is working!\")\n",
    "elif loss_reduction > 20:\n",
    "    print(\"   ‚ö† PARTIAL: Some learning occurred, but may need tuning\")\n",
    "else:\n",
    "    print(\"   ‚úó FAIL: Insufficient learning - check implementation!\")\n",
    "\n",
    "print(f\"2. Training Dataset:\")\n",
    "print(f\"   - Number of images: {NUM_SAMPLES}\")\n",
    "print(f\"   - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   - Learning rate: {model.lr}\")\n",
    "\n",
    "print(f\"3. Model Configuration:\")\n",
    "print(f\"   - Channels: {model.channels}\")\n",
    "print(f\"   - Number of scales: {model.num_scales}\")\n",
    "print(f\"   - Sigma range: [{model.sigmas[-1]:.4f}, {model.sigmas[0]:.4f}]\")\n",
    "\n",
    "print(\"4. Next Steps:\")\n",
    "if loss_reduction > 50:\n",
    "    print(\"   - Model can overfit successfully\")\n",
    "    print(\"   - Backpropagation is working correctly\")\n",
    "    print(\"   - Ready to train on full dataset\")\n",
    "    print(\"   - Consider: longer training, better hyperparameters, data normalization\")\n",
    "else:\n",
    "    print(\"   - Check loss function implementation\")\n",
    "    print(\"   - Verify gradient flow through the network\")\n",
    "    print(\"   - Review score matching target calculation\")\n",
    "    print(\"   - Check data normalization (should be [-1, 1])\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ],
   "id": "d5042b13c2d99a0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Test Score Matching Target Calculation\n",
    "Verify that the score target is computed correctly."
   ],
   "id": "40e9fef6552f4358"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test score target calculation\n",
    "print(\"Testing score matching target calculation...\")\n",
    "\n",
    "test_img = test_batch[0:1]\n",
    "sigma = model.sigmas[5]  # Mid-range sigma\n",
    "\n",
    "noise = torch.randn_like(test_img)\n",
    "perturbed = test_img + sigma * noise\n",
    "\n",
    "# The score of p(x_t | x_0) where x_t = x_0 + sigma * noise\n",
    "# is: ‚àá log p(x_t | x_0) = -(x_t - x_0) / sigma^2 = -noise / sigma\n",
    "target_score = -noise / sigma\n",
    "\n",
    "print(f\"Sigma value: {sigma.item():.4f}\")\n",
    "print(f\"Noise mean: {noise.mean().item():.6f}, std: {noise.std().item():.6f}\")\n",
    "print(f\"Target score mean: {target_score.mean().item():.6f}, std: {target_score.std().item():.6f}\")\n",
    "\n",
    "# Predict score\n",
    "with torch.no_grad():\n",
    "    sigma_input = torch.ones(1, device=device) * sigma\n",
    "    predicted_score = model.score_net(perturbed, sigma_input)\n",
    "\n",
    "print(f\"Predicted score mean: {predicted_score.mean().item():.6f}, std: {predicted_score.std().item():.6f}\")\n",
    "\n",
    "mse = ((predicted_score - target_score) ** 2).mean()\n",
    "print(f\"MSE between predicted and target: {mse.item():.6f}\")\n",
    "print(\"(Lower MSE after training indicates better score estimation)\")\n"
   ],
   "id": "a65b06cb9849cb7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 11. Recommendations Based on Results",
   "id": "7e3d0d05e508ca92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRAINING ANALYSIS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate statistics\n",
    "initial_loss = losses[0]\n",
    "final_loss = losses[-1]\n",
    "min_loss = min(losses)\n",
    "max_loss = max(losses)\n",
    "loss_reduction = (initial_loss - final_loss) / initial_loss * 100\n",
    "\n",
    "# Count instabilities\n",
    "instabilities = 0\n",
    "for i in range(1, len(losses)):\n",
    "    if losses[i] > losses[i-1] * 1.2:  # 20% increase\n",
    "        instabilities += 1\n",
    "\n",
    "print(f\"\\n1. Loss Statistics:\")\n",
    "print(f\"   Initial loss:  {initial_loss:.2f}\")\n",
    "print(f\"   Final loss:    {final_loss:.2f}\")\n",
    "print(f\"   Min loss:      {min_loss:.2f}\")\n",
    "print(f\"   Max loss:      {max_loss:.2f}\")\n",
    "print(f\"   Reduction:     {loss_reduction:.1f}%\")\n",
    "print(f\"   Instabilities: {instabilities} (loss increased by >20%)\")\n",
    "\n",
    "print(f\"\\n2. Assessment:\")\n",
    "if instabilities > 5:\n",
    "    print(\"   ‚ùå SEVERE INSTABILITY - Training is highly unstable\")\n",
    "    print(\"      ‚Üí Gradients are likely exploding\")\n",
    "    print(\"      ‚Üí Loss scale may be too large\")\n",
    "elif instabilities > 2:\n",
    "    print(\"   ‚ö†Ô∏è  MODERATE INSTABILITY - Some training instability detected\")\n",
    "    print(\"      ‚Üí Consider gradient clipping or lower learning rate\")\n",
    "else:\n",
    "    print(\"   ‚úì STABLE - Loss decreases smoothly\")\n",
    "\n",
    "if final_loss < 100:\n",
    "    print(\"   ‚úì EXCELLENT OVERFITTING - Model memorized training data\")\n",
    "elif final_loss < 500:\n",
    "    print(\"   ‚úì GOOD OVERFITTING - Model learning well\")\n",
    "elif final_loss < 1000:\n",
    "    print(\"   ‚ö†Ô∏è  PARTIAL OVERFITTING - More training needed\")\n",
    "else:\n",
    "    print(\"   ‚ùå POOR OVERFITTING - Model not learning effectively\")\n",
    "\n",
    "print(f\"\\n3. Recommended Actions:\")\n",
    "if instabilities > 2 or final_loss > 500:\n",
    "    print(\"   üìå CRITICAL FIXES:\")\n",
    "    if instabilities > 2:\n",
    "        print(\"      ‚Ä¢ Add/strengthen gradient clipping: max_norm=0.5 or 0.1\")\n",
    "        print(\"      ‚Ä¢ Reduce learning rate: try lr=1e-4 or 1e-5\")\n",
    "    if final_loss > 1000:\n",
    "        print(\"      ‚Ä¢ Try simplified loss: use_simple_loss=True\")\n",
    "        print(\"      ‚Ä¢ Check data normalization (should be [0,1])\")\n",
    "        print(\"      ‚Ä¢ Train for more epochs (1000+)\")\n",
    "\n",
    "    print(\"\\n   üí° DEBUGGING STEPS:\")\n",
    "    print(\"      1. Re-run with lr=1e-4 and max_norm=0.5\")\n",
    "    print(\"      2. Check gradient norms in section 4.5\")\n",
    "    print(\"      3. Monitor loss scale in section 4.6\")\n",
    "    print(\"      4. If still failing, try use_simple_loss=True\")\n",
    "else:\n",
    "    print(\"   ‚úì Training looks good! You can proceed to:\")\n",
    "    print(\"      ‚Ä¢ Train on full CIFAR-10 dataset\")\n",
    "    print(\"      ‚Ä¢ Experiment with hyperparameters\")\n",
    "    print(\"      ‚Ä¢ Generate more samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ],
   "id": "b82f7d80b967ab2b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
